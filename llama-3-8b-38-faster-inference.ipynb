{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c589fef1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.011681,
     "end_time": "2024-07-09T04:27:04.557075",
     "exception": false,
     "start_time": "2024-07-09T04:27:04.545394",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ðŸ¦™ðŸ¦™ðŸ¦™ What this notebook is\n",
    "This notebook is made upon [Inference - llama-3 8b](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b) by @kishanvavdara. If you haven't checked the linked notebook I highly recommend you to check and upvote.\n",
    "I made a few improvements upon @kishanvavdara's work:\n",
    "\n",
    "### 38% faster inference\n",
    "Inference time using the first 10k samples in the training set takes 40 mins using this script (without TTA) while the original script takes 65 mins, which is 38% faster without any degradation in accuracy. I mainly added two things:\n",
    "\n",
    "#### 1. Dynamic padding\n",
    "Instead of padding all the inputs to a fixed length in advance, padding is applied on-the-fly up to the longest sequence in each mini-batch.\n",
    "\n",
    "#### 2. Sort the test data by input length\n",
    "To take full advantage of dynamic padding, the test data is sorted by input length. This way, inputs in each mini-batch have more or less same length to reduce the redundant padding.\n",
    "\n",
    "### Longer input sequence\n",
    "Although 99% of the training data falls within 1024, the rest 1% are not. Besides, test set may have more long sequences, so I suppose it's safer to make `max_length` as long as possible.\n",
    "Changing `max_length` from 1024 to 1280 improved LB from 0.989 to 0.983.\n",
    "\n",
    "## Things I have tried but didn't work\n",
    "\n",
    "### Test Time Augmentation (TTA)\n",
    "I tried a simple TTA which swaps the order of response_a and response_b. Note that this will increase the inference time by 2x as model is called twice per sample.\n",
    "We can average the two softmax probabilities or average the two logits and then compute softmax probability. Alghouth both approaches didn't improve LB, averaging softmax performed better.\n",
    "TTA will increase the inference time 2x as model is called twice per sample. Submission finished within 9 hours with `max_length=1280` and TTA enabled thanks to the efficient inference.\n",
    "\n",
    "### Truncate each input\n",
    "The original implementation truncates the concatenated sequence i.e. prompt + response_a + response_b. Naively applying truncation may end up producing prompt only input as some (though rare) prompt is longer than 1280 tokens, then the model has no way but randomly guessing the winner.\n",
    "I tried to truncate each input to a fixed length first and then concatenate the three. But it didn't improve LB.\n",
    "\n",
    "## ðŸ†• Update in version 4\n",
    "The efficient inference gives us enough time to increase the input sequence length, so I changed `max_length` to 2048 while mini-batch size is reduced to 4 from 8.\n",
    "In addition, I enabled [Memory-Efficient Attention](https://github.com/facebookresearch/xformers) to reduce memory usage.\n",
    "This improved LB from 0.983 to 0.979 and submission still takes less then 4 hours without TTA.\n",
    "We can go even longer by reducing mini-batch size to 1 but I haven't tested yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05beb7b",
   "metadata": {},
   "source": [
    "Source: https://www.kaggle.com/code/emiz6413/llama-3-8b-38-faster-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec1cab",
   "metadata": {
    "papermill": {
     "duration": 0.011234,
     "end_time": "2024-07-09T04:27:04.579503",
     "exception": false,
     "start_time": "2024-07-09T04:27:04.568269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d96c20c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:27:04.604432Z",
     "iopub.status.busy": "2024-07-09T04:27:04.603690Z",
     "iopub.status.idle": "2024-07-09T04:27:57.445421Z",
     "shell.execute_reply": "2024-07-09T04:27:57.444357Z"
    },
    "papermill": {
     "duration": 52.857055,
     "end_time": "2024-07-09T04:27:57.447941",
     "exception": false,
     "start_time": "2024-07-09T04:27:04.590886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bec0608",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:27:57.472632Z",
     "iopub.status.busy": "2024-07-09T04:27:57.472301Z",
     "iopub.status.idle": "2024-07-09T04:28:15.070002Z",
     "shell.execute_reply": "2024-07-09T04:28:15.068981Z"
    },
    "papermill": {
     "duration": 17.612674,
     "end_time": "2024-07-09T04:28:15.072524",
     "exception": false,
     "start_time": "2024-07-09T04:27:57.459850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 04:28:07.330725: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-09 04:28:07.330834: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-09 04:28:07.438535: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6397da",
   "metadata": {
    "papermill": {
     "duration": 0.011135,
     "end_time": "2024-07-09T04:28:15.095810",
     "exception": false,
     "start_time": "2024-07-09T04:28:15.084675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "According to the pytorch [documentation](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html?highlight=scaled_dot_product#torch.nn.functional.scaled_dot_product_attention), `scaled_dot_product_attention` automatically select the most optimal implementation from:\n",
    "1. Flash Attention\n",
    "2. Memory Efficient Attention\n",
    "3. A PyTorch (naive) implementation\n",
    "\n",
    "By default, all of those are enabled but we can also manually enable/disable certain backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d749e345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:28:15.120149Z",
     "iopub.status.busy": "2024-07-09T04:28:15.119105Z",
     "iopub.status.idle": "2024-07-09T04:28:15.124185Z",
     "shell.execute_reply": "2024-07-09T04:28:15.123383Z"
    },
    "papermill": {
     "duration": 0.019135,
     "end_time": "2024-07-09T04:28:15.126047",
     "exception": false,
     "start_time": "2024-07-09T04:28:15.106912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.device_count() == 2, \"Sorry - multi-GPU required!\"\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_flash_sdp(True)  # Doesn't have any effect as Flash Attention does not support T4/P100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "340a5526",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:28:15.150503Z",
     "iopub.status.busy": "2024-07-09T04:28:15.150205Z",
     "iopub.status.idle": "2024-07-09T04:28:15.155580Z",
     "shell.execute_reply": "2024-07-09T04:28:15.154750Z"
    },
    "papermill": {
     "duration": 0.019488,
     "end_time": "2024-07-09T04:28:15.157554",
     "exception": false,
     "start_time": "2024-07-09T04:28:15.138066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "    weights_path = '/kaggle/input/lmsys-model/model'\n",
    "    max_length = 2048\n",
    "    batch_size = 4\n",
    "    device = torch.device(\"cuda\")    \n",
    "    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>\n",
    "    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e65d0",
   "metadata": {
    "papermill": {
     "duration": 0.011251,
     "end_time": "2024-07-09T04:28:15.179808",
     "exception": false,
     "start_time": "2024-07-09T04:28:15.168557",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e272c99a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:28:15.203506Z",
     "iopub.status.busy": "2024-07-09T04:28:15.203215Z",
     "iopub.status.idle": "2024-07-09T04:28:15.233068Z",
     "shell.execute_reply": "2024-07-09T04:28:15.232220Z"
    },
    "papermill": {
     "duration": 0.044033,
     "end_time": "2024-07-09T04:28:15.235160",
     "exception": false,
     "start_time": "2024-07-09T04:28:15.191127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>I have three oranges today, I ate an orange ye...</td>\n",
       "      <td>You have two oranges today.</td>\n",
       "      <td>You still have three oranges. Eating an orange...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>You are a mediator in a heated political debat...</td>\n",
       "      <td>Thank you for sharing the details of the situa...</td>\n",
       "      <td>Mr Reddy and Ms Blue both have valid points in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>How to initialize the classification head when...</td>\n",
       "      <td>When you want to initialize the classification...</td>\n",
       "      <td>To initialize the classification head when per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             prompt  \\\n",
       "0   136060  I have three oranges today, I ate an orange ye...   \n",
       "1   211333  You are a mediator in a heated political debat...   \n",
       "2  1233961  How to initialize the classification head when...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0                        You have two oranges today.   \n",
       "1  Thank you for sharing the details of the situa...   \n",
       "2  When you want to initialize the classification...   \n",
       "\n",
       "                                          response_b  \n",
       "0  You still have three oranges. Eating an orange...  \n",
       "1  Mr Reddy and Ms Blue both have valid points in...  \n",
       "2  To initialize the classification head when per...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "\n",
    "# concatenate strings in list\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "display(test.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0f614a",
   "metadata": {
    "papermill": {
     "duration": 0.011131,
     "end_time": "2024-07-09T04:28:15.257877",
     "exception": false,
     "start_time": "2024-07-09T04:28:15.246746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a37fbb91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:28:15.281510Z",
     "iopub.status.busy": "2024-07-09T04:28:15.281251Z",
     "iopub.status.idle": "2024-07-09T04:28:15.289420Z",
     "shell.execute_reply": "2024-07-09T04:28:15.288631Z"
    },
    "papermill": {
     "duration": 0.0221,
     "end_time": "2024-07-09T04:28:15.291357",
     "exception": false,
     "start_time": "2024-07-09T04:28:15.269257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(\n",
    "    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n",
    "):\n",
    "    prompt = [\"User prompt: \" + p for p in prompt]\n",
    "    response_a = [\"\\n\\nModel A :\\n\" + r_a for r_a in response_a]\n",
    "    response_b = [\"\\n\\n--------\\n\\nModel B:\\n\" + r_b for r_b in response_b]\n",
    "    if spread_max_length:\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]\n",
    "    else:\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n",
    "        input_ids = tokenized.input_ids\n",
    "        attention_mask = tokenized.attention_mask\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da0933a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:28:15.315417Z",
     "iopub.status.busy": "2024-07-09T04:28:15.315174Z",
     "iopub.status.idle": "2024-07-09T04:28:15.883816Z",
     "shell.execute_reply": "2024-07-09T04:28:15.882836Z"
    },
    "papermill": {
     "duration": 0.582606,
     "end_time": "2024-07-09T04:28:15.885816",
     "exception": false,
     "start_time": "2024-07-09T04:28:15.303210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 420 ms, sys: 63.7 ms, total: 484 ms\n",
      "Wall time: 562 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)\n",
    "\n",
    "aug_data = pd.DataFrame()\n",
    "aug_data[\"id\"] = test[\"id\"]\n",
    "# swap response_a & response_b\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d011587a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:28:15.910565Z",
     "iopub.status.busy": "2024-07-09T04:28:15.910254Z",
     "iopub.status.idle": "2024-07-09T04:28:15.915770Z",
     "shell.execute_reply": "2024-07-09T04:28:15.914853Z"
    },
    "papermill": {
     "duration": 0.020138,
     "end_time": "2024-07-09T04:28:15.917845",
     "exception": false,
     "start_time": "2024-07-09T04:28:15.897707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User prompt: I have three oranges today, I ate an orange yesterday. How many oranges do I have?\n",
      "\n",
      "Model A :\n",
      "You have two oranges today.\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "You still have three oranges. Eating an orange yesterday does not affect the number of oranges you have today.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05438f33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:28:15.941856Z",
     "iopub.status.busy": "2024-07-09T04:28:15.941586Z",
     "iopub.status.idle": "2024-07-09T04:28:15.946840Z",
     "shell.execute_reply": "2024-07-09T04:28:15.945985Z"
    },
    "papermill": {
     "duration": 0.019562,
     "end_time": "2024-07-09T04:28:15.948911",
     "exception": false,
     "start_time": "2024-07-09T04:28:15.929349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User prompt: I have three oranges today, I ate an orange yesterday. How many oranges do I have?\n",
      "\n",
      "Model A :\n",
      "You still have three oranges. Eating an orange yesterday does not affect the number of oranges you have today.\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "You have two oranges today.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(aug_data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0f1d24",
   "metadata": {
    "papermill": {
     "duration": 0.011447,
     "end_time": "2024-07-09T04:28:15.972145",
     "exception": false,
     "start_time": "2024-07-09T04:28:15.960698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load model \n",
    "We load 1 model on each gpu.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82e1e90b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:28:15.997113Z",
     "iopub.status.busy": "2024-07-09T04:28:15.996853Z",
     "iopub.status.idle": "2024-07-09T04:30:02.632240Z",
     "shell.execute_reply": "2024-07-09T04:30:02.631466Z"
    },
    "papermill": {
     "duration": 106.650526,
     "end_time": "2024-07-09T04:30:02.634317",
     "exception": false,
     "start_time": "2024-07-09T04:28:15.983791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397a5a0e259541399586ddffa2fcdf31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/llama-3/transformers/8b-chat-hf/1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8963a7a86b254bc58c32edb4c5fe9b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/llama-3/transformers/8b-chat-hf/1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# BitsAndBytes configuration\n",
    "bnb_config =  BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16,\n",
    "    bnb_8bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load base model on GPU 0\n",
    "device_0 = torch.device('cuda:0')\n",
    "base_model_0 = LlamaForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:0')\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Load base model on GPU 1\n",
    "device_1 = torch.device('cuda:1')\n",
    "base_model_1 = LlamaForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:1')\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d1d7d",
   "metadata": {
    "papermill": {
     "duration": 0.012647,
     "end_time": "2024-07-09T04:30:02.659523",
     "exception": false,
     "start_time": "2024-07-09T04:30:02.646876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "470304ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:30:02.685156Z",
     "iopub.status.busy": "2024-07-09T04:30:02.684878Z",
     "iopub.status.idle": "2024-07-09T04:30:02.689519Z",
     "shell.execute_reply": "2024-07-09T04:30:02.688620Z"
    },
    "papermill": {
     "duration": 0.019839,
     "end_time": "2024-07-09T04:30:02.691430",
     "exception": false,
     "start_time": "2024-07-09T04:30:02.671591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.10,\n",
    "    bias='none',\n",
    "    inference_mode=True,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules=['o_proj', 'v_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c303712d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:30:02.716828Z",
     "iopub.status.busy": "2024-07-09T04:30:02.716569Z",
     "iopub.status.idle": "2024-07-09T04:30:15.993094Z",
     "shell.execute_reply": "2024-07-09T04:30:15.992217Z"
    },
    "papermill": {
     "duration": 13.291482,
     "end_time": "2024-07-09T04:30:15.995146",
     "exception": false,
     "start_time": "2024-07-09T04:30:02.703664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=1024, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=3, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=3, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get peft\n",
    "model_0 = get_peft_model(base_model_0, peft_config).to(device_0) \n",
    "# Load weights\n",
    "model_0.load_state_dict(torch.load(cfg.weights_path), strict=False)\n",
    "model_0.eval()\n",
    "\n",
    "model_1 = get_peft_model(base_model_1, peft_config).to(device_1)\n",
    "model_1.load_state_dict(torch.load(cfg.weights_path), strict=False)\n",
    "model_1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c3b3dda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:30:16.021786Z",
     "iopub.status.busy": "2024-07-09T04:30:16.021454Z",
     "iopub.status.idle": "2024-07-09T04:30:16.033860Z",
     "shell.execute_reply": "2024-07-09T04:30:16.033011Z"
    },
    "papermill": {
     "duration": 0.027931,
     "end_time": "2024-07-09T04:30:16.035845",
     "exception": false,
     "start_time": "2024-07-09T04:30:16.007914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 24,576 || all params: 7,511,764,992 || trainable%: 0.00032716678471934814\n",
      "trainable params: 24,576 || all params: 7,511,764,992 || trainable%: 0.00032716678471934814\n"
     ]
    }
   ],
   "source": [
    "# Trainable Parameters\n",
    "model_0.print_trainable_parameters()\n",
    "model_1.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9874556a",
   "metadata": {
    "papermill": {
     "duration": 0.012475,
     "end_time": "2024-07-09T04:30:16.060957",
     "exception": false,
     "start_time": "2024-07-09T04:30:16.048482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0445f16c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:30:16.087390Z",
     "iopub.status.busy": "2024-07-09T04:30:16.087120Z",
     "iopub.status.idle": "2024-07-09T04:30:16.095947Z",
     "shell.execute_reply": "2024-07-09T04:30:16.095243Z"
    },
    "papermill": {
     "duration": 0.024076,
     "end_time": "2024-07-09T04:30:16.097913",
     "exception": false,
     "start_time": "2024-07-09T04:30:16.073837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))\n",
    "        proba = outputs.logits.softmax(-1).cpu()\n",
    "        \n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win\n",
    "    df[\"winner_model_b\"] = b_win\n",
    "    df[\"winner_tie\"] = tie\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66608e3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:30:16.123850Z",
     "iopub.status.busy": "2024-07-09T04:30:16.123569Z",
     "iopub.status.idle": "2024-07-09T04:30:19.265999Z",
     "shell.execute_reply": "2024-07-09T04:30:19.263991Z"
    },
    "papermill": {
     "duration": 3.159623,
     "end_time": "2024-07-09T04:30:19.270126",
     "exception": false,
     "start_time": "2024-07-09T04:30:16.110503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 3.1345057487487793\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "\n",
    "# sort by input length to fully leverage dynaminc padding\n",
    "data = data.sort_values(\"length\", ascending=False)\n",
    "# the total #tokens in sub_1 and sub_2 should be more or less the same\n",
    "sub_1 = data.iloc[0::2].copy()\n",
    "sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba0298bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:30:19.300620Z",
     "iopub.status.busy": "2024-07-09T04:30:19.300299Z",
     "iopub.status.idle": "2024-07-09T04:30:19.308285Z",
     "shell.execute_reply": "2024-07-09T04:30:19.307404Z"
    },
    "papermill": {
     "duration": 0.024202,
     "end_time": "2024-07-09T04:30:19.310398",
     "exception": false,
     "start_time": "2024-07-09T04:30:19.286196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 0.00022101402282714844\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "\n",
    "if cfg.tta:\n",
    "    data = aug_data.sort_values(\"length\", ascending=False)  # sort by input length to boost speed\n",
    "    sub_1 = data.iloc[0::2].copy()\n",
    "    sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "    tta_result_df = pd.concat(list(results), axis=0)\n",
    "    # recall TTA's order is flipped\n",
    "    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n",
    "    # average original result and TTA result.\n",
    "    proba = (proba + tta_proba) / 2\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5bd1b5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:30:19.337154Z",
     "iopub.status.busy": "2024-07-09T04:30:19.336914Z",
     "iopub.status.idle": "2024-07-09T04:30:19.352687Z",
     "shell.execute_reply": "2024-07-09T04:30:19.351873Z"
    },
    "papermill": {
     "duration": 0.031083,
     "end_time": "2024-07-09T04:30:19.354679",
     "exception": false,
     "start_time": "2024-07-09T04:30:19.323596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>0.209280</td>\n",
       "      <td>0.463401</td>\n",
       "      <td>0.327319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>0.106557</td>\n",
       "      <td>0.656575</td>\n",
       "      <td>0.236869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>0.349609</td>\n",
       "      <td>0.367821</td>\n",
       "      <td>0.282570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  winner_model_a  winner_model_b  winner_tie\n",
       "2  1233961        0.209280        0.463401    0.327319\n",
       "0   136060        0.106557        0.656575    0.236869\n",
       "1   211333        0.349609        0.367821    0.282570"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "display(submission_df)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5034873,
     "sourceId": 8449074,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 200.2354,
   "end_time": "2024-07-09T04:30:22.063333",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-09T04:27:01.827933",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "007f3a37106c43f292a2b16470497a87": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1dd72873df8c41839ef97013209d2dd2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fdcd3a1411df4b6a8f8b6f0e3646440b",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3fd65abc05fe44bf93faa8bfc801dc98",
       "value": 4
      }
     },
     "27992189853d4d8cbe6a02b4a51f8e1c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_007f3a37106c43f292a2b16470497a87",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_dda1a740f60843298c01f8117cdccf95",
       "value": " 4/4 [01:32&lt;00:00, 18.66s/it]"
      }
     },
     "283ffec198b040aab8da128ea1ace497": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "36f64d460d164401aa2bb40a855c9322": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "397a5a0e259541399586ddffa2fcdf31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c00b4ffc8e614c09b4f2eb082b643247",
        "IPY_MODEL_5682bfe7bc6146eca73438d73b91728d",
        "IPY_MODEL_27992189853d4d8cbe6a02b4a51f8e1c"
       ],
       "layout": "IPY_MODEL_9dd1644040264114a9222dad326378e9"
      }
     },
     "3fd65abc05fe44bf93faa8bfc801dc98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5682bfe7bc6146eca73438d73b91728d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_283ffec198b040aab8da128ea1ace497",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_aa6a557f0ed94d689cdc032978afa65f",
       "value": 4
      }
     },
     "5af6d4a6a9934cb990a8a8cc428b0c59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e636b00fc90c4dfb9090e2cf3cacff10",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_5ca4e883840741c88fd0c78bce492ad5",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "5ca4e883840741c88fd0c78bce492ad5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8963a7a86b254bc58c32edb4c5fe9b8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5af6d4a6a9934cb990a8a8cc428b0c59",
        "IPY_MODEL_1dd72873df8c41839ef97013209d2dd2",
        "IPY_MODEL_a06834be798649b2be25d5533a2f6457"
       ],
       "layout": "IPY_MODEL_9d952147c7644365888e989f268c59b6"
      }
     },
     "9d952147c7644365888e989f268c59b6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9dd1644040264114a9222dad326378e9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a06834be798649b2be25d5533a2f6457": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ae6be89cc40b451a9d3d6a417cb94c95",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_b5b9c05c03594d72a3b4b4e4b94c0902",
       "value": " 4/4 [00:13&lt;00:00,  2.70s/it]"
      }
     },
     "aa6a557f0ed94d689cdc032978afa65f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ae6be89cc40b451a9d3d6a417cb94c95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b5b9c05c03594d72a3b4b4e4b94c0902": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c00b4ffc8e614c09b4f2eb082b643247": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_36f64d460d164401aa2bb40a855c9322",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_d2d9db5bde7547fbb4fa6305068b3923",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "d2d9db5bde7547fbb4fa6305068b3923": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "dda1a740f60843298c01f8117cdccf95": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e636b00fc90c4dfb9090e2cf3cacff10": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fdcd3a1411df4b6a8f8b6f0e3646440b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
